{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7241600,"sourceType":"datasetVersion","datasetId":4194368}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch_geometric","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-21T20:01:34.325468Z","iopub.execute_input":"2023-12-21T20:01:34.325802Z","iopub.status.idle":"2023-12-21T20:01:49.022330Z","shell.execute_reply.started":"2023-12-21T20:01:34.325774Z","shell.execute_reply":"2023-12-21T20:01:49.021270Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Obtaining dependency information for torch_geometric from https://files.pythonhosted.org/packages/65/4e/6f9a75548a93fedcd4514ae2de9bee1e91bade6b73252b4da32f0e42ac52/torch_geometric-2.4.0-py3-none-any.whl.metadata\n  Downloading torch_geometric-2.4.0-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.66.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.24.3)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.11.4)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.31.0)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.0.9)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.2.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2023.11.17)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (3.2.0)\nDownloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.4.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\nimport torch\nimport torch_geometric \nfrom torch_geometric.data import Dataset, Data, DataLoader\nfrom torch_geometric.utils import to_dense_batch\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\ndf = pd.read_csv('/kaggle/input/inn-competition/Trainset-Evaluation/Train_Set.csv')\nts = pd.read_csv('/kaggle/input/inn-competition/Transaction/InnoTech_Trans.csv')\n# For prototype phase we remove the nan values\nts=ts.dropna()\nts = ts.reset_index().drop(columns='index')\n\n# TEMP LINE CODE\ndf = df.drop_duplicates(subset=['CARD'])\n\nts = ts.drop(columns=['Terminal_ID'])","metadata":{"execution":{"iopub.status.busy":"2023-12-21T20:01:49.024964Z","iopub.execute_input":"2023-12-21T20:01:49.025368Z","iopub.status.idle":"2023-12-21T20:02:06.686294Z","shell.execute_reply.started":"2023-12-21T20:01:49.025322Z","shell.execute_reply":"2023-12-21T20:02:06.685456Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"le = LabelEncoder()  \nle.fit(np.unique(np.concatenate([ts['Primary_ID'].values,ts['Second_ID'].values,df['CARD'].values])).tolist())\n\nts['Primary_ID'] = le.transform(ts['Primary_ID'])\nts['Second_ID'] = le.transform(ts['Second_ID'])\ndf['CARD'] = le.transform(df['CARD'])\n\n# ts = pd.get_dummies(ts, columns=['Date'], prefix='Date')","metadata":{"execution":{"iopub.status.busy":"2023-12-21T20:02:06.687453Z","iopub.execute_input":"2023-12-21T20:02:06.687759Z","iopub.status.idle":"2023-12-21T20:02:32.132840Z","shell.execute_reply.started":"2023-12-21T20:02:06.687733Z","shell.execute_reply":"2023-12-21T20:02:32.132025Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nencoder = OneHotEncoder(sparse=False, drop='first')  # 'first' to drop one of the columns to avoid multicollinearity\n\ncolumn_transformer = ColumnTransformer(\n    transformers=[('onehot', encoder, ['Date'])],\n    remainder='passthrough'  # keep the other columns as they are\n)\n\ndf_encoded = pd.DataFrame(column_transformer.fit_transform(ts))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-21T20:02:32.133943Z","iopub.execute_input":"2023-12-21T20:02:32.134227Z","iopub.status.idle":"2023-12-21T20:02:37.326360Z","shell.execute_reply.started":"2023-12-21T20:02:32.134203Z","shell.execute_reply":"2023-12-21T20:02:37.325391Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"ts=pd.concat([ts,df_encoded],axis=1).drop(columns=['Date'])","metadata":{"execution":{"iopub.status.busy":"2023-12-21T20:02:37.328717Z","iopub.execute_input":"2023-12-21T20:02:37.329062Z","iopub.status.idle":"2023-12-21T20:02:41.659242Z","shell.execute_reply.started":"2023-12-21T20:02:37.329033Z","shell.execute_reply":"2023-12-21T20:02:41.658404Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch_geometric.data import Data\n\ndef construct_graph(card_id,y):\n    \"\"\"\n    This function will create a graph of transaction for each card base on the transaction dataset. The created graph\n    is directed which means each CARD_ID can be source or destination of the transaction.\n\n    params:\n        card_id \n        ts: transaction dataset\n\n    return:\n\n        graph \n    \"\"\"\n\n    card_to_index = ts[ts['Primary_ID']==card_id].index\n    card_in_index = ts[ts['Second_ID']==card_id].index\n    \n    edge_index_from = []\n    edge_index_to = []\n    \n    node_features = [np.ones(104)]\n    \n    for i,index in zip(card_to_index,range(1,len(card_to_index)+1)):\n        node_features.append(ts.loc[i].values)\n        edge_index_from.append(0)\n        edge_index_to.append(index)\n\n    for i,index in zip(card_in_index,range(len(card_to_index)+1, \n                                           len(card_to_index)+len(card_in_index)+2+1)):\n        node_features.append(ts.loc[i].values)\n        edge_index_from.append(index)\n        edge_index_to.append(0)\n\n    data = Data(x=torch.tensor(node_features), edge_index=torch.tensor([edge_index_from,edge_index_to]),y=y)\n\n    return data\n","metadata":{"execution":{"iopub.status.busy":"2023-12-21T20:02:41.660388Z","iopub.execute_input":"2023-12-21T20:02:41.660689Z","iopub.status.idle":"2023-12-21T20:02:41.669248Z","shell.execute_reply.started":"2023-12-21T20:02:41.660665Z","shell.execute_reply":"2023-12-21T20:02:41.668364Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nfrom multiprocessing import Pool\n\ndef process_data(args):\n    i, y = args\n    return construct_graph(i, y)\n\ndata = []\n\n# Number of processes to use (adjust as needed)\nnum_processes = 4\n\n# Create a pool of processes\nwith Pool(num_processes) as pool:\n    # Use tqdm with imap_unordered to show progress\n    for result in tqdm(pool.imap_unordered(process_data, zip(df['CARD'], df['LABEL']), chunksize=1), total=len(df)):\n        data.append(result)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-21T20:02:41.670530Z","iopub.execute_input":"2023-12-21T20:02:41.670855Z","iopub.status.idle":"2023-12-21T20:04:18.664885Z","shell.execute_reply.started":"2023-12-21T20:02:41.670825Z","shell.execute_reply":"2023-12-21T20:04:18.663717Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"  0%|          | 0/3235 [00:00<?, ?it/s]/tmp/ipykernel_42/287827871.py:37: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n  data = Data(x=torch.tensor(node_features), edge_index=torch.tensor([edge_index_from,edge_index_to]),y=y)\n/tmp/ipykernel_42/287827871.py:37: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n  data = Data(x=torch.tensor(node_features), edge_index=torch.tensor([edge_index_from,edge_index_to]),y=y)\n/tmp/ipykernel_42/287827871.py:37: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n  data = Data(x=torch.tensor(node_features), edge_index=torch.tensor([edge_index_from,edge_index_to]),y=y)\n  0%|          | 6/3235 [00:00<02:05, 25.68it/s]/tmp/ipykernel_42/287827871.py:37: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n  data = Data(x=torch.tensor(node_features), edge_index=torch.tensor([edge_index_from,edge_index_to]),y=y)\n100%|██████████| 3235/3235 [01:36<00:00, 33.40it/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the prepared data\ntorch.save(data,'/kaggle/working/graph_data.pt')","metadata":{"execution":{"iopub.status.busy":"2023-12-21T20:04:18.666427Z","iopub.execute_input":"2023-12-21T20:04:18.666834Z","iopub.status.idle":"2023-12-21T20:04:20.315249Z","shell.execute_reply.started":"2023-12-21T20:04:18.666789Z","shell.execute_reply":"2023-12-21T20:04:20.314408Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# import os\n# import torch\n# from torch_geometric.data import Dataset, Data\n# from torch_geometric.utils import to_dense_batch\n\n# class CustomGraphDataset(Dataset):\n#     def __init__(self, root, transform=None):\n#         self.root = root\n#         self.transform = transform\n#         self.graph_files =root\n#         self.data = torch.load(self.graph_files)\n#         self.max_num_nodes = self.find_max_num_nodes()\n#         self.max_num_edge_index = self.find_max_num_edge_index()\n        \n#     def find_max_num_edge_index(self):\n#         max_num_edge_index = max(data.edge_index.shape[1] for data in self.data)\n#         return max_num_edge_index\n    \n#     def find_max_num_nodes(self):\n#         max_num_nodes = max(data.num_nodes for data in self.data)\n#         return max_num_nodes\n    \n#     def indices(self):\n#         return list(range(self.len()))\n\n#     def len(self):\n#         return len(self.data)\n\n#     def get(self, idx):\n        \n#         num_nodes_to_pad = self.max_num_nodes - self.data[idx].num_nodes\n#         num_edge_to_pad = self.max_num_edge_index - self.data[idx].edge_index.shape[1]\n        \n#         # Pad num_nodes with zeros\n#         self.data[idx].x = torch.cat([self.data[idx].x, torch.zeros(num_nodes_to_pad)])\n        \n#         # Pad edge_index with zeros\n#         self.data[idx].edge_index = torch.cat([self.data[idx].edge_index, torch.zeros(2,num_edge_to_pad)],dim=1)\n#         self.data[idx].edge_attr = torch.cat([self.data[idx].edge_attr,torch.zeros(num_edge_to_pad,104)])\n#         # Update the number of nodes in the data object\n#         self.data[idx].num_nodes = self.max_num_nodes\n\n#         return self.data[idx]","metadata":{"execution":{"iopub.status.busy":"2023-12-19T22:57:40.352389Z","iopub.execute_input":"2023-12-19T22:57:40.353124Z","iopub.status.idle":"2023-12-19T22:57:40.366064Z","shell.execute_reply.started":"2023-12-19T22:57:40.353078Z","shell.execute_reply":"2023-12-19T22:57:40.364053Z"},"trusted":true},"execution_count":201,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from torch_geometric.data import Data\n# from torch_geometric.utils import to_dense_batch\n\n# def collate_fn(batch):\n#     batch = to_dense_batch(batch)\n#     return Data(x=batch.x, edge_index=batch.edge_index, y=batch.y)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T22:57:40.511033Z","iopub.execute_input":"2023-12-19T22:57:40.511402Z","iopub.status.idle":"2023-12-19T22:57:40.516215Z","shell.execute_reply.started":"2023-12-19T22:57:40.511377Z","shell.execute_reply":"2023-12-19T22:57:40.515341Z"},"trusted":true},"execution_count":202,"outputs":[]},{"cell_type":"code","source":"# root = '/kaggle/working/graph_data.pt'\n# train_dataset = CustomGraphDataset(root)\n# train_dataset.max_num_nodes","metadata":{"execution":{"iopub.status.busy":"2023-12-19T22:57:40.696505Z","iopub.execute_input":"2023-12-19T22:57:40.696902Z","iopub.status.idle":"2023-12-19T22:57:41.300915Z","shell.execute_reply.started":"2023-12-19T22:57:40.696873Z","shell.execute_reply":"2023-12-19T22:57:41.299477Z"},"trusted":true},"execution_count":203,"outputs":[{"execution_count":203,"output_type":"execute_result","data":{"text/plain":"24651"},"metadata":{}}]},{"cell_type":"code","source":"from torch_geometric.loader import DataLoader\n\n# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n# # test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# # for step, data in enumerate(train_loader):\n# #     print(f'Step {step + 1}:')\n# #     print('=======')\n# #     print(f'Number of graphs in the current batch: {data.num_graphs}')\n# #     print(data)\n# #     print()","metadata":{"execution":{"iopub.status.busy":"2023-12-21T20:05:20.106007Z","iopub.execute_input":"2023-12-21T20:05:20.106730Z","iopub.status.idle":"2023-12-21T20:05:20.111035Z","shell.execute_reply.started":"2023-12-21T20:05:20.106696Z","shell.execute_reply":"2023-12-21T20:05:20.110180Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from torch_geometric.loader import DataLoader\n\ntrain_loader = DataLoader(data, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-21T20:05:24.687850Z","iopub.execute_input":"2023-12-21T20:05:24.688592Z","iopub.status.idle":"2023-12-21T20:05:24.693420Z","shell.execute_reply.started":"2023-12-21T20:05:24.688561Z","shell.execute_reply":"2023-12-21T20:05:24.692450Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def weights_init(m):\n    if isinstance(m, torch.nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight.data)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-21T20:05:26.390019Z","iopub.execute_input":"2023-12-21T20:05:26.390732Z","iopub.status.idle":"2023-12-21T20:05:26.395202Z","shell.execute_reply.started":"2023-12-21T20:05:26.390699Z","shell.execute_reply":"2023-12-21T20:05:26.394249Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from torch.nn import Linear\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.nn import global_mean_pool\n\n\nfrom torch.nn import Linear\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GATConv\nfrom torch_geometric.nn import global_mean_pool\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Linear\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom torch.nn import BatchNorm1d  # Corrected import\n\nclass GCN(torch.nn.Module):\n    def __init__(self, hidden_channels):\n        super(GCN, self).__init__()\n        torch.manual_seed(12345)\n        self.conv1 = GCNConv(104, hidden_channels)\n        self.bn1 = BatchNorm1d(hidden_channels)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.bn2 = BatchNorm1d(hidden_channels)\n        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n        self.bn3 = BatchNorm1d(hidden_channels)\n        self.lin = Linear(hidden_channels, 3)\n\n    def forward(self, x, edge_index, batch):\n        # 1. Obtain node embeddings \n        x = self.conv1(x, edge_index)\n        x = self.bn1(x)\n        x = x.relu()\n        x = self.conv2(x, edge_index)\n        x = self.bn2(x)\n        x = x.relu()\n        x = self.conv3(x, edge_index)\n        x = self.bn3(x)\n\n        # 2. Readout layer\n        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n\n        # 3. Apply a final classifier\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.lin(x)\n        \n        return x\n\n\n\nmodel = GCN(64)\n\nmodel.apply(weights_init)\nprint(model)\nmodel.cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ncriterion = torch.nn.CrossEntropyLoss()\n\ndef train():\n    model.train()\n\n    for data in train_loader:  # Iterate in batches over the training dataset.\n#          print(data.x.shape)\n#          print(data.edge_index.shape)\n#          print(data.edge_attr.shape)\n#          print(data.x.shape)\n         data.cuda()\n         out = model(data.x.float(), data.edge_index.int(), data.batch)  # Perform a single forward pass.\n         loss = criterion(out, data.y.long())  # Compute the loss.\n         loss.backward()  # Derive gradients.\n         optimizer.step()  # Update parameters based on gradients.\n         optimizer.zero_grad()  # Clear gradients.\n\ndef test(loader):\n     model.eval()\n     correct = 0\n     for data in loader: # Iterate in batches over the training/test dataset.\n         data.cuda()\n         out = model(data.x.float(), data.edge_index.int(), data.batch)  # Perform a single forward pass.\n         pred = out.argmax(dim=1)  # Use the class with highest probability.\n         correct += int((pred == data.y.long()).sum())  # Check against ground-truth labels.\n     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n","metadata":{"execution":{"iopub.status.busy":"2023-12-21T20:09:04.570498Z","iopub.execute_input":"2023-12-21T20:09:04.571524Z","iopub.status.idle":"2023-12-21T20:09:04.598131Z","shell.execute_reply.started":"2023-12-21T20:09:04.571459Z","shell.execute_reply":"2023-12-21T20:09:04.597027Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"GCN(\n  (conv1): GCNConv(104, 64)\n  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv2): GCNConv(64, 64)\n  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv3): GCNConv(64, 64)\n  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (lin): Linear(in_features=64, out_features=3, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(1, 171):\n    train()\n    train_acc = test(train_loader)\n#     test_acc = test(test_loader)\n    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-21T20:09:05.676096Z","iopub.execute_input":"2023-12-21T20:09:05.676798Z","iopub.status.idle":"2023-12-21T20:13:42.325735Z","shell.execute_reply.started":"2023-12-21T20:09:05.676767Z","shell.execute_reply":"2023-12-21T20:13:42.324582Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch: 001, Train Acc: 0.6343\nEpoch: 002, Train Acc: 0.5153\nEpoch: 003, Train Acc: 0.2974\nEpoch: 004, Train Acc: 0.6352\nEpoch: 005, Train Acc: 0.4192\nEpoch: 006, Train Acc: 0.5666\nEpoch: 007, Train Acc: 0.6921\nEpoch: 008, Train Acc: 0.4485\nEpoch: 009, Train Acc: 0.6705\nEpoch: 010, Train Acc: 0.6788\nEpoch: 011, Train Acc: 0.6454\nEpoch: 012, Train Acc: 0.6556\nEpoch: 013, Train Acc: 0.6665\nEpoch: 014, Train Acc: 0.6798\nEpoch: 015, Train Acc: 0.6247\nEpoch: 016, Train Acc: 0.6594\nEpoch: 017, Train Acc: 0.6782\nEpoch: 018, Train Acc: 0.6779\nEpoch: 019, Train Acc: 0.6192\nEpoch: 020, Train Acc: 0.6798\nEpoch: 021, Train Acc: 0.6538\nEpoch: 022, Train Acc: 0.6764\nEpoch: 023, Train Acc: 0.4578\nEpoch: 024, Train Acc: 0.7221\nEpoch: 025, Train Acc: 0.7221\nEpoch: 026, Train Acc: 0.6655\nEpoch: 027, Train Acc: 0.5372\nEpoch: 028, Train Acc: 0.7592\nEpoch: 029, Train Acc: 0.7017\nEpoch: 030, Train Acc: 0.7178\nEpoch: 031, Train Acc: 0.6807\nEpoch: 032, Train Acc: 0.6896\nEpoch: 033, Train Acc: 0.6396\nEpoch: 034, Train Acc: 0.6878\nEpoch: 035, Train Acc: 0.7162\nEpoch: 036, Train Acc: 0.6838\nEpoch: 037, Train Acc: 0.7113\nEpoch: 038, Train Acc: 0.7141\nEpoch: 039, Train Acc: 0.6903\nEpoch: 040, Train Acc: 0.6581\nEpoch: 041, Train Acc: 0.6983\nEpoch: 042, Train Acc: 0.7172\nEpoch: 043, Train Acc: 0.7376\nEpoch: 044, Train Acc: 0.7045\nEpoch: 045, Train Acc: 0.7085\nEpoch: 046, Train Acc: 0.7329\nEpoch: 047, Train Acc: 0.7323\nEpoch: 048, Train Acc: 0.7505\nEpoch: 049, Train Acc: 0.7029\nEpoch: 050, Train Acc: 0.6556\nEpoch: 051, Train Acc: 0.6943\nEpoch: 052, Train Acc: 0.7209\nEpoch: 053, Train Acc: 0.6937\nEpoch: 054, Train Acc: 0.7512\nEpoch: 055, Train Acc: 0.7400\nEpoch: 056, Train Acc: 0.6856\nEpoch: 057, Train Acc: 0.7518\nEpoch: 058, Train Acc: 0.7499\nEpoch: 059, Train Acc: 0.7354\nEpoch: 060, Train Acc: 0.7413\nEpoch: 061, Train Acc: 0.7261\nEpoch: 062, Train Acc: 0.7110\nEpoch: 063, Train Acc: 0.7326\nEpoch: 064, Train Acc: 0.7261\nEpoch: 065, Train Acc: 0.7648\nEpoch: 066, Train Acc: 0.7128\nEpoch: 067, Train Acc: 0.7376\nEpoch: 068, Train Acc: 0.7218\nEpoch: 069, Train Acc: 0.7267\nEpoch: 070, Train Acc: 0.7586\nEpoch: 071, Train Acc: 0.7487\nEpoch: 072, Train Acc: 0.7379\nEpoch: 073, Train Acc: 0.7369\nEpoch: 074, Train Acc: 0.7363\nEpoch: 075, Train Acc: 0.7567\nEpoch: 076, Train Acc: 0.7354\nEpoch: 077, Train Acc: 0.7577\nEpoch: 078, Train Acc: 0.7304\nEpoch: 079, Train Acc: 0.7400\nEpoch: 080, Train Acc: 0.7722\nEpoch: 081, Train Acc: 0.7611\nEpoch: 082, Train Acc: 0.7577\nEpoch: 083, Train Acc: 0.6189\nEpoch: 084, Train Acc: 0.7509\nEpoch: 085, Train Acc: 0.7564\nEpoch: 086, Train Acc: 0.7549\nEpoch: 087, Train Acc: 0.7440\nEpoch: 088, Train Acc: 0.7348\nEpoch: 089, Train Acc: 0.6900\nEpoch: 090, Train Acc: 0.7657\nEpoch: 091, Train Acc: 0.6934\nEpoch: 092, Train Acc: 0.7626\nEpoch: 093, Train Acc: 0.7104\nEpoch: 094, Train Acc: 0.7029\nEpoch: 095, Train Acc: 0.7184\nEpoch: 096, Train Acc: 0.7555\nEpoch: 097, Train Acc: 0.7611\nEpoch: 098, Train Acc: 0.7428\nEpoch: 099, Train Acc: 0.7478\nEpoch: 100, Train Acc: 0.7410\nEpoch: 101, Train Acc: 0.7301\nEpoch: 102, Train Acc: 0.7478\nEpoch: 103, Train Acc: 0.7428\nEpoch: 104, Train Acc: 0.7206\nEpoch: 105, Train Acc: 0.7391\nEpoch: 106, Train Acc: 0.7484\nEpoch: 107, Train Acc: 0.7586\nEpoch: 108, Train Acc: 0.7567\nEpoch: 109, Train Acc: 0.7224\nEpoch: 110, Train Acc: 0.7654\nEpoch: 111, Train Acc: 0.7611\nEpoch: 112, Train Acc: 0.7304\nEpoch: 113, Train Acc: 0.7422\nEpoch: 114, Train Acc: 0.7348\nEpoch: 115, Train Acc: 0.7573\nEpoch: 116, Train Acc: 0.7376\nEpoch: 117, Train Acc: 0.7431\nEpoch: 118, Train Acc: 0.7474\nEpoch: 119, Train Acc: 0.7422\nEpoch: 120, Train Acc: 0.7512\nEpoch: 121, Train Acc: 0.7329\nEpoch: 122, Train Acc: 0.7258\nEpoch: 123, Train Acc: 0.7326\nEpoch: 124, Train Acc: 0.7292\nEpoch: 125, Train Acc: 0.7561\nEpoch: 126, Train Acc: 0.7345\nEpoch: 127, Train Acc: 0.7184\nEpoch: 128, Train Acc: 0.6989\nEpoch: 129, Train Acc: 0.7369\nEpoch: 130, Train Acc: 0.7629\nEpoch: 131, Train Acc: 0.7261\nEpoch: 132, Train Acc: 0.7224\nEpoch: 133, Train Acc: 0.7088\nEpoch: 134, Train Acc: 0.7212\nEpoch: 135, Train Acc: 0.6893\nEpoch: 136, Train Acc: 0.7824\nEpoch: 137, Train Acc: 0.7422\nEpoch: 138, Train Acc: 0.7215\nEpoch: 139, Train Acc: 0.6866\nEpoch: 140, Train Acc: 0.7561\nEpoch: 141, Train Acc: 0.7706\nEpoch: 142, Train Acc: 0.7116\nEpoch: 143, Train Acc: 0.7379\nEpoch: 144, Train Acc: 0.7543\nEpoch: 145, Train Acc: 0.7459\nEpoch: 146, Train Acc: 0.7666\nEpoch: 147, Train Acc: 0.7734\nEpoch: 148, Train Acc: 0.7440\nEpoch: 149, Train Acc: 0.7264\nEpoch: 150, Train Acc: 0.7527\nEpoch: 151, Train Acc: 0.7419\nEpoch: 152, Train Acc: 0.7456\nEpoch: 153, Train Acc: 0.7632\nEpoch: 154, Train Acc: 0.7462\nEpoch: 155, Train Acc: 0.7703\nEpoch: 156, Train Acc: 0.7716\nEpoch: 157, Train Acc: 0.7552\nEpoch: 158, Train Acc: 0.7515\nEpoch: 159, Train Acc: 0.7478\nEpoch: 160, Train Acc: 0.7413\nEpoch: 161, Train Acc: 0.7487\nEpoch: 162, Train Acc: 0.7014\nEpoch: 163, Train Acc: 0.7515\nEpoch: 164, Train Acc: 0.7558\nEpoch: 165, Train Acc: 0.7595\nEpoch: 166, Train Acc: 0.7428\nEpoch: 167, Train Acc: 0.7014\nEpoch: 168, Train Acc: 0.7697\nEpoch: 169, Train Acc: 0.7416\nEpoch: 170, Train Acc: 0.7765\n","output_type":"stream"}]},{"cell_type":"code","source":"22137*104","metadata":{"execution":{"iopub.status.busy":"2023-12-20T18:01:04.784313Z","iopub.execute_input":"2023-12-20T18:01:04.784954Z","iopub.status.idle":"2023-12-20T18:01:04.796451Z","shell.execute_reply.started":"2023-12-20T18:01:04.784912Z","shell.execute_reply":"2023-12-20T18:01:04.793279Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"2302248"},"metadata":{}}]},{"cell_type":"code","source":"from torch.nn import Linear\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.nn import global_mean_pool\n\n\nfrom torch.nn import Linear\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GATConv\nfrom torch_geometric.nn import global_mean_pool\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Linear\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom torch.nn import BatchNorm1d  # Corrected import\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Linear, BatchNorm1d\nfrom torch_geometric.nn import global_mean_pool, GATConv\n\nclass GATSkip(torch.nn.Module):\n    def __init__(self, hidden_channels, heads=1):\n        super(GATSkip, self).__init__()\n        torch.manual_seed(12345)\n        self.conv1 = GATConv(104, hidden_channels, heads=heads)\n        self.bn1 = BatchNorm1d(hidden_channels)\n        self.conv2 = GATConv(hidden_channels, hidden_channels, heads=heads)\n        self.bn2 = BatchNorm1d(hidden_channels)\n        self.conv3 = GATConv(hidden_channels, hidden_channels, heads=heads)\n        self.bn3 = BatchNorm1d(hidden_channels)\n        self.lin = Linear(hidden_channels, 3)\n\n    def forward(self, x, edge_index, batch):\n        # 1. Obtain node embeddings with skip connections\n        x1 = self.conv1(x, edge_index)\n        x1 = self.bn1(x1)\n        x1 = F.relu(x1)\n\n        x2 = self.conv2(x1, edge_index)\n        x2 = self.bn2(x2)\n        x2 = F.relu(x2)\n\n        x3 = self.conv3(x2, edge_index)\n        x3 = self.bn3(x3)\n\n        # Skip connections\n        x = x1 + x2 + x3\n\n        # 2. Readout layer\n        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n\n        # 3. Apply a final classifier\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.lin(x)\n\n        return x\n\n\n\nmodel = GCN(256)\n\nmodel.apply(weights_init)\nprint(model)\nmodel.cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ncriterion = torch.nn.CrossEntropyLoss()\n\ndef train():\n    model.train()\n\n    for data in train_loader:  # Iterate in batches over the training dataset.\n#          print(data.x.shape)\n#          print(data.edge_index.shape)\n#          print(data.edge_attr.shape)\n#          print(data.x.shape)\n         data.cuda()\n         out = model(data.x.float(), data.edge_index.int(), data.batch)  # Perform a single forward pass.\n         loss = criterion(out, data.y.long())  # Compute the loss.\n         loss.backward()  # Derive gradients.\n         optimizer.step()  # Update parameters based on gradients.\n         optimizer.zero_grad()  # Clear gradients.\n\ndef test(loader):\n     model.eval()\n     correct = 0\n     for data in loader: # Iterate in batches over the training/test dataset.\n         data.cuda()\n         out = model(data.x.float(), data.edge_index.int(), data.batch)  # Perform a single forward pass.\n         pred = out.argmax(dim=1)  # Use the class with highest probability.\n         correct += int((pred == data.y.long()).sum())  # Check against ground-truth labels.\n     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n","metadata":{"execution":{"iopub.status.busy":"2023-12-21T20:24:04.992516Z","iopub.execute_input":"2023-12-21T20:24:04.993372Z","iopub.status.idle":"2023-12-21T20:24:05.017234Z","shell.execute_reply.started":"2023-12-21T20:24:04.993335Z","shell.execute_reply":"2023-12-21T20:24:05.016227Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"GCN(\n  (conv1): GCNConv(104, 256)\n  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv2): GCNConv(256, 256)\n  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv3): GCNConv(256, 256)\n  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (lin): Linear(in_features=256, out_features=3, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(1, 171):\n    train()\n    train_acc = test(train_loader)\n#     test_acc = test(test_loader)\n    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-21T20:24:08.450269Z","iopub.execute_input":"2023-12-21T20:24:08.451021Z","iopub.status.idle":"2023-12-21T20:30:45.866310Z","shell.execute_reply.started":"2023-12-21T20:24:08.450987Z","shell.execute_reply":"2023-12-21T20:30:45.865194Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch: 001, Train Acc: 0.5471\nEpoch: 002, Train Acc: 0.5839\nEpoch: 003, Train Acc: 0.2856\nEpoch: 004, Train Acc: 0.2983\nEpoch: 005, Train Acc: 0.5376\nEpoch: 006, Train Acc: 0.4862\nEpoch: 007, Train Acc: 0.6269\nEpoch: 008, Train Acc: 0.6986\nEpoch: 009, Train Acc: 0.6646\nEpoch: 010, Train Acc: 0.6862\nEpoch: 011, Train Acc: 0.6958\nEpoch: 012, Train Acc: 0.4662\nEpoch: 013, Train Acc: 0.6575\nEpoch: 014, Train Acc: 0.6612\nEpoch: 015, Train Acc: 0.6386\nEpoch: 016, Train Acc: 0.7076\nEpoch: 017, Train Acc: 0.6479\nEpoch: 018, Train Acc: 0.7113\nEpoch: 019, Train Acc: 0.6563\nEpoch: 020, Train Acc: 0.6921\nEpoch: 021, Train Acc: 0.5051\nEpoch: 022, Train Acc: 0.7008\nEpoch: 023, Train Acc: 0.7131\nEpoch: 024, Train Acc: 0.6971\nEpoch: 025, Train Acc: 0.7147\nEpoch: 026, Train Acc: 0.5750\nEpoch: 027, Train Acc: 0.6989\nEpoch: 028, Train Acc: 0.6903\nEpoch: 029, Train Acc: 0.7199\nEpoch: 030, Train Acc: 0.6383\nEpoch: 031, Train Acc: 0.6825\nEpoch: 032, Train Acc: 0.7264\nEpoch: 033, Train Acc: 0.7422\nEpoch: 034, Train Acc: 0.7054\nEpoch: 035, Train Acc: 0.7066\nEpoch: 036, Train Acc: 0.6906\nEpoch: 037, Train Acc: 0.6195\nEpoch: 038, Train Acc: 0.7141\nEpoch: 039, Train Acc: 0.7128\nEpoch: 040, Train Acc: 0.6739\nEpoch: 041, Train Acc: 0.6844\nEpoch: 042, Train Acc: 0.6878\nEpoch: 043, Train Acc: 0.7264\nEpoch: 044, Train Acc: 0.7456\nEpoch: 045, Train Acc: 0.7221\nEpoch: 046, Train Acc: 0.7252\nEpoch: 047, Train Acc: 0.7230\nEpoch: 048, Train Acc: 0.6668\nEpoch: 049, Train Acc: 0.7131\nEpoch: 050, Train Acc: 0.7286\nEpoch: 051, Train Acc: 0.7391\nEpoch: 052, Train Acc: 0.7134\nEpoch: 053, Train Acc: 0.7385\nEpoch: 054, Train Acc: 0.7131\nEpoch: 055, Train Acc: 0.7212\nEpoch: 056, Train Acc: 0.6918\nEpoch: 057, Train Acc: 0.7292\nEpoch: 058, Train Acc: 0.7110\nEpoch: 059, Train Acc: 0.7338\nEpoch: 060, Train Acc: 0.7493\nEpoch: 061, Train Acc: 0.7292\nEpoch: 062, Train Acc: 0.7335\nEpoch: 063, Train Acc: 0.7082\nEpoch: 064, Train Acc: 0.5849\nEpoch: 065, Train Acc: 0.7070\nEpoch: 066, Train Acc: 0.6977\nEpoch: 067, Train Acc: 0.7440\nEpoch: 068, Train Acc: 0.6754\nEpoch: 069, Train Acc: 0.7394\nEpoch: 070, Train Acc: 0.6869\nEpoch: 071, Train Acc: 0.7626\nEpoch: 072, Train Acc: 0.7116\nEpoch: 073, Train Acc: 0.7632\nEpoch: 074, Train Acc: 0.7502\nEpoch: 075, Train Acc: 0.7042\nEpoch: 076, Train Acc: 0.7564\nEpoch: 077, Train Acc: 0.7252\nEpoch: 078, Train Acc: 0.5957\nEpoch: 079, Train Acc: 0.7580\nEpoch: 080, Train Acc: 0.7369\nEpoch: 081, Train Acc: 0.7682\nEpoch: 082, Train Acc: 0.7233\nEpoch: 083, Train Acc: 0.7629\nEpoch: 084, Train Acc: 0.7440\nEpoch: 085, Train Acc: 0.7635\nEpoch: 086, Train Acc: 0.7654\nEpoch: 087, Train Acc: 0.7638\nEpoch: 088, Train Acc: 0.7471\nEpoch: 089, Train Acc: 0.7100\nEpoch: 090, Train Acc: 0.7645\nEpoch: 091, Train Acc: 0.7536\nEpoch: 092, Train Acc: 0.7376\nEpoch: 093, Train Acc: 0.7394\nEpoch: 094, Train Acc: 0.7218\nEpoch: 095, Train Acc: 0.7379\nEpoch: 096, Train Acc: 0.7366\nEpoch: 097, Train Acc: 0.7150\nEpoch: 098, Train Acc: 0.6918\nEpoch: 099, Train Acc: 0.7020\nEpoch: 100, Train Acc: 0.6980\nEpoch: 101, Train Acc: 0.7246\nEpoch: 102, Train Acc: 0.7583\nEpoch: 103, Train Acc: 0.7289\nEpoch: 104, Train Acc: 0.7468\nEpoch: 105, Train Acc: 0.7172\nEpoch: 106, Train Acc: 0.7604\nEpoch: 107, Train Acc: 0.7549\nEpoch: 108, Train Acc: 0.7774\nEpoch: 109, Train Acc: 0.7502\nEpoch: 110, Train Acc: 0.7512\nEpoch: 111, Train Acc: 0.7286\nEpoch: 112, Train Acc: 0.7391\nEpoch: 113, Train Acc: 0.7107\nEpoch: 114, Train Acc: 0.7456\nEpoch: 115, Train Acc: 0.7518\nEpoch: 116, Train Acc: 0.7694\nEpoch: 117, Train Acc: 0.7165\nEpoch: 118, Train Acc: 0.7570\nEpoch: 119, Train Acc: 0.7450\nEpoch: 120, Train Acc: 0.7632\nEpoch: 121, Train Acc: 0.7444\nEpoch: 122, Train Acc: 0.7342\nEpoch: 123, Train Acc: 0.7592\nEpoch: 124, Train Acc: 0.7694\nEpoch: 125, Train Acc: 0.7119\nEpoch: 126, Train Acc: 0.7564\nEpoch: 127, Train Acc: 0.7638\nEpoch: 128, Train Acc: 0.7719\nEpoch: 129, Train Acc: 0.7515\nEpoch: 130, Train Acc: 0.7682\nEpoch: 131, Train Acc: 0.7509\nEpoch: 132, Train Acc: 0.6662\nEpoch: 133, Train Acc: 0.7453\nEpoch: 134, Train Acc: 0.7641\nEpoch: 135, Train Acc: 0.7499\nEpoch: 136, Train Acc: 0.7641\nEpoch: 137, Train Acc: 0.7682\nEpoch: 138, Train Acc: 0.7700\nEpoch: 139, Train Acc: 0.7372\nEpoch: 140, Train Acc: 0.7737\nEpoch: 141, Train Acc: 0.7474\nEpoch: 142, Train Acc: 0.7512\nEpoch: 143, Train Acc: 0.7632\nEpoch: 144, Train Acc: 0.7691\nEpoch: 145, Train Acc: 0.7756\nEpoch: 146, Train Acc: 0.7743\nEpoch: 147, Train Acc: 0.7459\nEpoch: 148, Train Acc: 0.7230\nEpoch: 149, Train Acc: 0.7116\nEpoch: 150, Train Acc: 0.7509\nEpoch: 151, Train Acc: 0.7648\nEpoch: 152, Train Acc: 0.7326\nEpoch: 153, Train Acc: 0.7781\nEpoch: 154, Train Acc: 0.7428\nEpoch: 155, Train Acc: 0.7264\nEpoch: 156, Train Acc: 0.7583\nEpoch: 157, Train Acc: 0.7666\nEpoch: 158, Train Acc: 0.7320\nEpoch: 159, Train Acc: 0.7824\nEpoch: 160, Train Acc: 0.7493\nEpoch: 161, Train Acc: 0.7703\nEpoch: 162, Train Acc: 0.7694\nEpoch: 163, Train Acc: 0.7233\nEpoch: 164, Train Acc: 0.7026\nEpoch: 165, Train Acc: 0.7570\nEpoch: 166, Train Acc: 0.7626\nEpoch: 167, Train Acc: 0.7478\nEpoch: 168, Train Acc: 0.7505\nEpoch: 169, Train Acc: 0.7706\nEpoch: 170, Train Acc: 0.7697\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.nn import Linear\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.nn import global_mean_pool\n\n\nfrom torch.nn import Linear\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GATConv\nfrom torch_geometric.nn import global_mean_pool\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Linear\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom torch.nn import BatchNorm1d  # Corrected import\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Linear, BatchNorm1d\nfrom torch_geometric.nn import global_mean_pool, GATConv\n\nclass GATSkip(torch.nn.Module):\n    def __init__(self, hidden_channels, heads=1):\n        super(GATSkip, self).__init__()\n        torch.manual_seed(12345)\n        self.conv1 = GATConv(104, hidden_channels, heads=heads)\n        self.bn1 = BatchNorm1d(hidden_channels)\n        self.conv2 = GATConv(hidden_channels, hidden_channels, heads=heads)\n        self.bn2 = BatchNorm1d(hidden_channels)\n        self.conv3 = GATConv(hidden_channels, hidden_channels, heads=heads)\n        self.bn3 = BatchNorm1d(hidden_channels)\n        self.lin = Linear(hidden_channels, 3)\n\n    def forward(self, x, edge_index, batch):\n        # 1. Obtain node embeddings with skip connections\n        x1 = self.conv1(x, edge_index)\n        x1 = self.bn1(x1)\n        x1 = F.elu(x1)\n\n        x2 = self.conv2(x1, edge_index)\n        x2 = self.bn2(x2)\n        x2 = F.elu(x2)\n\n        x3 = self.conv3(x2, edge_index)\n        x3 = self.bn3(x3)\n\n        # Skip connections\n        x = x1 + x2 + x3\n\n        # 2. Readout layer\n        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n\n        # 3. Apply a final classifier\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.lin(x)\n\n        return x\n\n\n\nmodel = GCN(256)\n\nmodel.apply(weights_init)\nprint(model)\nmodel.cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ncriterion = torch.nn.CrossEntropyLoss()\n\ndef train():\n    model.train()\n\n    for data in train_loader:  # Iterate in batches over the training dataset.\n#          print(data.x.shape)\n#          print(data.edge_index.shape)\n#          print(data.edge_attr.shape)\n#          print(data.x.shape)\n         data.cuda()\n         out = model(data.x.float(), data.edge_index.int(), data.batch)  # Perform a single forward pass.\n         loss = criterion(out, data.y.long())  # Compute the loss.\n         loss.backward()  # Derive gradients.\n         optimizer.step()  # Update parameters based on gradients.\n         optimizer.zero_grad()  # Clear gradients.\n\ndef test(loader):\n     model.eval()\n     correct = 0\n     for data in loader: # Iterate in batches over the training/test dataset.\n         data.cuda()\n         out = model(data.x.float(), data.edge_index.int(), data.batch)  # Perform a single forward pass.\n         pred = out.argmax(dim=1)  # Use the class with highest probability.\n         correct += int((pred == data.y.long()).sum())  # Check against ground-truth labels.\n     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n","metadata":{"execution":{"iopub.status.busy":"2023-12-21T20:35:35.834897Z","iopub.execute_input":"2023-12-21T20:35:35.835290Z","iopub.status.idle":"2023-12-21T20:35:35.862425Z","shell.execute_reply.started":"2023-12-21T20:35:35.835260Z","shell.execute_reply":"2023-12-21T20:35:35.861511Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"GCN(\n  (conv1): GCNConv(104, 256)\n  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv2): GCNConv(256, 256)\n  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv3): GCNConv(256, 256)\n  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (lin): Linear(in_features=256, out_features=3, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(1, 171):\n    train()\n    train_acc = test(train_loader)\n#     test_acc = test(test_loader)\n    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-21T20:35:45.685420Z","iopub.execute_input":"2023-12-21T20:35:45.685818Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch: 001, Train Acc: 0.5481\nEpoch: 002, Train Acc: 0.5861\nEpoch: 003, Train Acc: 0.3184\nEpoch: 004, Train Acc: 0.3054\nEpoch: 005, Train Acc: 0.3376\nEpoch: 006, Train Acc: 0.4436\nEpoch: 007, Train Acc: 0.6272\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}